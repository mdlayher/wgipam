// Code generated by command: go run gen.go -avx. DO NOT EDIT.

#include "textflag.h"

DATA prime_avx<>+0(SB)/4, $0x9e3779b1
GLOBL prime_avx<>(SB), RODATA|NOPTR, $4

// func accum_avx(acc *[8]uint64, data *byte, key *byte, len uint64)
TEXT Â·accum_avx(SB), NOSPLIT, $0-32
	MOVQ         acc+0(FP), AX
	MOVQ         data+8(FP), CX
	MOVQ         key+16(FP), DX
	MOVQ         key+16(FP), BX
	MOVQ         len+24(FP), BP
	VMOVDQU      (AX), Y1
	VMOVDQU      32(AX), Y2
	VPBROADCASTQ prime_avx<>+0(SB), Y0

accum_large:
	CMPQ     BP, $0x00000400
	JLT      accum
	VMOVDQU  (CX), Y3
	VPXOR    (DX), Y3, Y4
	VPSHUFD  $0xf5, Y4, Y5
	VPMULUDQ Y5, Y4, Y4
	VPADDQ   Y3, Y4, Y3
	VPADDQ   Y1, Y3, Y1
	VMOVDQU  32(CX), Y3
	VPXOR    32(DX), Y3, Y4
	VPSHUFD  $0xf5, Y4, Y5
	VPMULUDQ Y5, Y4, Y4
	VPADDQ   Y3, Y4, Y3
	VPADDQ   Y2, Y3, Y2
	VMOVDQU  64(CX), Y3
	VPXOR    8(DX), Y3, Y4
	VPSHUFD  $0xf5, Y4, Y5
	VPMULUDQ Y5, Y4, Y4
	VPADDQ   Y3, Y4, Y3
	VPADDQ   Y1, Y3, Y1
	VMOVDQU  96(CX), Y3
	VPXOR    40(DX), Y3, Y4
	VPSHUFD  $0xf5, Y4, Y5
	VPMULUDQ Y5, Y4, Y4
	VPADDQ   Y3, Y4, Y3
	VPADDQ   Y2, Y3, Y2
	VMOVDQU  128(CX), Y3
	VPXOR    16(DX), Y3, Y4
	VPSHUFD  $0xf5, Y4, Y5
	VPMULUDQ Y5, Y4, Y4
	VPADDQ   Y3, Y4, Y3
	VPADDQ   Y1, Y3, Y1
	VMOVDQU  160(CX), Y3
	VPXOR    48(DX), Y3, Y4
	VPSHUFD  $0xf5, Y4, Y5
	VPMULUDQ Y5, Y4, Y4
	VPADDQ   Y3, Y4, Y3
	VPADDQ   Y2, Y3, Y2
	VMOVDQU  192(CX), Y3
	VPXOR    24(DX), Y3, Y4
	VPSHUFD  $0xf5, Y4, Y5
	VPMULUDQ Y5, Y4, Y4
	VPADDQ   Y3, Y4, Y3
	VPADDQ   Y1, Y3, Y1
	VMOVDQU  224(CX), Y3
	VPXOR    56(DX), Y3, Y4
	VPSHUFD  $0xf5, Y4, Y5
	VPMULUDQ Y5, Y4, Y4
	VPADDQ   Y3, Y4, Y3
	VPADDQ   Y2, Y3, Y2
	VMOVDQU  256(CX), Y3
	VPXOR    32(DX), Y3, Y4
	VPSHUFD  $0xf5, Y4, Y5
	VPMULUDQ Y5, Y4, Y4
	VPADDQ   Y3, Y4, Y3
	VPADDQ   Y1, Y3, Y1
	VMOVDQU  288(CX), Y3
	VPXOR    64(DX), Y3, Y4
	VPSHUFD  $0xf5, Y4, Y5
	VPMULUDQ Y5, Y4, Y4
	VPADDQ   Y3, Y4, Y3
	VPADDQ   Y2, Y3, Y2
	VMOVDQU  320(CX), Y3
	VPXOR    40(DX), Y3, Y4
	VPSHUFD  $0xf5, Y4, Y5
	VPMULUDQ Y5, Y4, Y4
	VPADDQ   Y3, Y4, Y3
	VPADDQ   Y1, Y3, Y1
	VMOVDQU  352(CX), Y3
	VPXOR    72(DX), Y3, Y4
	VPSHUFD  $0xf5, Y4, Y5
	VPMULUDQ Y5, Y4, Y4
	VPADDQ   Y3, Y4, Y3
	VPADDQ   Y2, Y3, Y2
	VMOVDQU  384(CX), Y3
	VPXOR    48(DX), Y3, Y4
	VPSHUFD  $0xf5, Y4, Y5
	VPMULUDQ Y5, Y4, Y4
	VPADDQ   Y3, Y4, Y3
	VPADDQ   Y1, Y3, Y1
	VMOVDQU  416(CX), Y3
	VPXOR    80(DX), Y3, Y4
	VPSHUFD  $0xf5, Y4, Y5
	VPMULUDQ Y5, Y4, Y4
	VPADDQ   Y3, Y4, Y3
	VPADDQ   Y2, Y3, Y2
	VMOVDQU  448(CX), Y3
	VPXOR    56(DX), Y3, Y4
	VPSHUFD  $0xf5, Y4, Y5
	VPMULUDQ Y5, Y4, Y4
	VPADDQ   Y3, Y4, Y3
	VPADDQ   Y1, Y3, Y1
	VMOVDQU  480(CX), Y3
	VPXOR    88(DX), Y3, Y4
	VPSHUFD  $0xf5, Y4, Y5
	VPMULUDQ Y5, Y4, Y4
	VPADDQ   Y3, Y4, Y3
	VPADDQ   Y2, Y3, Y2
	VMOVDQU  512(CX), Y3
	VPXOR    64(DX), Y3, Y4
	VPSHUFD  $0xf5, Y4, Y5
	VPMULUDQ Y5, Y4, Y4
	VPADDQ   Y3, Y4, Y3
	VPADDQ   Y1, Y3, Y1
	VMOVDQU  544(CX), Y3
	VPXOR    96(DX), Y3, Y4
	VPSHUFD  $0xf5, Y4, Y5
	VPMULUDQ Y5, Y4, Y4
	VPADDQ   Y3, Y4, Y3
	VPADDQ   Y2, Y3, Y2
	VMOVDQU  576(CX), Y3
	VPXOR    72(DX), Y3, Y4
	VPSHUFD  $0xf5, Y4, Y5
	VPMULUDQ Y5, Y4, Y4
	VPADDQ   Y3, Y4, Y3
	VPADDQ   Y1, Y3, Y1
	VMOVDQU  608(CX), Y3
	VPXOR    104(DX), Y3, Y4
	VPSHUFD  $0xf5, Y4, Y5
	VPMULUDQ Y5, Y4, Y4
	VPADDQ   Y3, Y4, Y3
	VPADDQ   Y2, Y3, Y2
	VMOVDQU  640(CX), Y3
	VPXOR    80(DX), Y3, Y4
	VPSHUFD  $0xf5, Y4, Y5
	VPMULUDQ Y5, Y4, Y4
	VPADDQ   Y3, Y4, Y3
	VPADDQ   Y1, Y3, Y1
	VMOVDQU  672(CX), Y3
	VPXOR    112(DX), Y3, Y4
	VPSHUFD  $0xf5, Y4, Y5
	VPMULUDQ Y5, Y4, Y4
	VPADDQ   Y3, Y4, Y3
	VPADDQ   Y2, Y3, Y2
	VMOVDQU  704(CX), Y3
	VPXOR    88(DX), Y3, Y4
	VPSHUFD  $0xf5, Y4, Y5
	VPMULUDQ Y5, Y4, Y4
	VPADDQ   Y3, Y4, Y3
	VPADDQ   Y1, Y3, Y1
	VMOVDQU  736(CX), Y3
	VPXOR    120(DX), Y3, Y4
	VPSHUFD  $0xf5, Y4, Y5
	VPMULUDQ Y5, Y4, Y4
	VPADDQ   Y3, Y4, Y3
	VPADDQ   Y2, Y3, Y2
	VMOVDQU  768(CX), Y3
	VPXOR    96(DX), Y3, Y4
	VPSHUFD  $0xf5, Y4, Y5
	VPMULUDQ Y5, Y4, Y4
	VPADDQ   Y3, Y4, Y3
	VPADDQ   Y1, Y3, Y1
	VMOVDQU  800(CX), Y3
	VPXOR    128(DX), Y3, Y4
	VPSHUFD  $0xf5, Y4, Y5
	VPMULUDQ Y5, Y4, Y4
	VPADDQ   Y3, Y4, Y3
	VPADDQ   Y2, Y3, Y2
	VMOVDQU  832(CX), Y3
	VPXOR    104(DX), Y3, Y4
	VPSHUFD  $0xf5, Y4, Y5
	VPMULUDQ Y5, Y4, Y4
	VPADDQ   Y3, Y4, Y3
	VPADDQ   Y1, Y3, Y1
	VMOVDQU  864(CX), Y3
	VPXOR    136(DX), Y3, Y4
	VPSHUFD  $0xf5, Y4, Y5
	VPMULUDQ Y5, Y4, Y4
	VPADDQ   Y3, Y4, Y3
	VPADDQ   Y2, Y3, Y2
	VMOVDQU  896(CX), Y3
	VPXOR    112(DX), Y3, Y4
	VPSHUFD  $0xf5, Y4, Y5
	VPMULUDQ Y5, Y4, Y4
	VPADDQ   Y3, Y4, Y3
	VPADDQ   Y1, Y3, Y1
	VMOVDQU  928(CX), Y3
	VPXOR    144(DX), Y3, Y4
	VPSHUFD  $0xf5, Y4, Y5
	VPMULUDQ Y5, Y4, Y4
	VPADDQ   Y3, Y4, Y3
	VPADDQ   Y2, Y3, Y2
	VMOVDQU  960(CX), Y3
	VPXOR    120(DX), Y3, Y4
	VPSHUFD  $0xf5, Y4, Y5
	VPMULUDQ Y5, Y4, Y4
	VPADDQ   Y3, Y4, Y3
	VPADDQ   Y1, Y3, Y1
	VMOVDQU  992(CX), Y3
	VPXOR    152(DX), Y3, Y4
	VPSHUFD  $0xf5, Y4, Y5
	VPMULUDQ Y5, Y4, Y4
	VPADDQ   Y3, Y4, Y3
	VPADDQ   Y2, Y3, Y2
	ADDQ     $0x00000400, CX
	SUBQ     $0x00000400, BP
	VPSRLQ   $0x2f, Y1, Y3
	VPXOR    Y1, Y3, Y3
	VPXOR    128(DX), Y3, Y3
	VPMULUDQ Y0, Y3, Y1
	VPSHUFD  $0xf5, Y3, Y3
	VPMULUDQ Y0, Y3, Y3
	VPSLLQ   $0x20, Y3, Y3
	VPADDQ   Y1, Y3, Y1
	VPSRLQ   $0x2f, Y2, Y3
	VPXOR    Y2, Y3, Y3
	VPXOR    160(DX), Y3, Y3
	VPMULUDQ Y0, Y3, Y2
	VPSHUFD  $0xf5, Y3, Y3
	VPMULUDQ Y0, Y3, Y3
	VPSLLQ   $0x20, Y3, Y3
	VPADDQ   Y2, Y3, Y2
	JMP      accum_large

accum:
	CMPQ     BP, $0x40
	JLT      finalize
	VMOVDQU  (CX), Y0
	VPXOR    (BX), Y0, Y3
	VPSHUFD  $0xf5, Y3, Y4
	VPMULUDQ Y4, Y3, Y3
	VPADDQ   Y0, Y3, Y0
	VPADDQ   Y1, Y0, Y1
	VMOVDQU  32(CX), Y0
	VPXOR    32(BX), Y0, Y3
	VPSHUFD  $0xf5, Y3, Y4
	VPMULUDQ Y4, Y3, Y3
	VPADDQ   Y0, Y3, Y0
	VPADDQ   Y2, Y0, Y2
	ADDQ     $0x00000040, CX
	SUBQ     $0x00000040, BP
	ADDQ     $0x00000008, BX
	JMP      accum

finalize:
	CMPQ     BP, $0x00
	JE       return
	SUBQ     $0x40, CX
	ADDQ     BP, CX
	VMOVDQU  (CX), Y0
	VPXOR    121(DX), Y0, Y3
	VPSHUFD  $0xf5, Y3, Y4
	VPMULUDQ Y4, Y3, Y3
	VPADDQ   Y0, Y3, Y0
	VPADDQ   Y1, Y0, Y1
	VMOVDQU  32(CX), Y0
	VPXOR    153(DX), Y0, Y3
	VPSHUFD  $0xf5, Y3, Y4
	VPMULUDQ Y4, Y3, Y3
	VPADDQ   Y0, Y3, Y0
	VPADDQ   Y2, Y0, Y2

return:
	VMOVDQU Y1, (AX)
	VMOVDQU Y2, 32(AX)
	RET
